# ğŸ¥ AI-Powered Video Search with CLIP

Search for specific scenes in videos using natural language queries in **English or Arabic**! This project uses OpenAI's CLIP model to find frames matching your text description from either uploaded videos or YouTube links.


## ğŸŒŸ Features

- ğŸ” **Semantic Video Search**: Find specific scenes using natural language
- ğŸŒ **YouTube Support**: Search directly from YouTube videos without downloading
- ğŸ“ **Local Video Upload**: Upload videos from your device
- ğŸŒ **Multilingual**: Supports both English and Arabic queries
- ğŸ¯ **CLIP-Powered**: Uses OpenAI's CLIP for accurate image-text matching
- âš¡ **Fast Processing**: Efficient frame extraction and embedding
- ğŸ¨ **User-Friendly Interface**: Built with Gradio for easy interaction

## ğŸš€ Demo

Search for "apple" or "ØªÙØ§Ø­" in a video and get relevant frames instantly!

```
Query: "girl singing" â†’ Returns frames with people singing
Query: "sunset" â†’ Returns frames with sunset scenes
Query: "ØªÙØ§Ø­" â†’ Returns frames with apples (Arabic support!)
```

## ğŸ“‹ Requirements

```bash
Python 3.8+
CUDA-compatible GPU (optional, but recommended)
```

## ğŸ› ï¸ Installation

### 1. Clone the repository

```bash
git clone https://github.com/YOUR_USERNAME/video-search-clip.git
cd video-search-clip
```

### 2. Create a virtual environment (recommended)

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

## ğŸ“¦ Dependencies

Create a `requirements.txt` file with:

```
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0
gradio>=4.0.0
opencv-python>=4.8.0
yt-dlp>=2023.10.0
Pillow>=10.0.0
numpy>=1.24.0
sentencepiece>=0.1.99
protobuf>=3.20.0
```

## ğŸ® Usage

### Run the application

```bash
python app.py
```

The Gradio interface will launch in your browser at `http://localhost:7860`

### Using the Interface

1. **Input Source**:
   - Upload a video from your device, **OR**
   - Paste a YouTube URL

2. **Search Query**: 
   - Enter your search term in English or Arabic
   - Examples: "car driving", "sunset", "ØªÙØ§Ø­", "Ø¨Ù†Øª Ø¨ØªØºÙ†ÙŠ"

3. **Top-K Results**: 
   - Select how many matching frames you want (1-10)

4. **Click Search**: 
   - Get the most relevant frames with similarity scores



## ğŸ§  How It Works

1. **Frame Extraction**: Extracts frames from video (1 frame per second by default)
2. **Image Embedding**: Converts frames to CLIP embeddings
3. **Text Embedding**: Converts search query to CLIP embedding (with Arabicâ†’English translation if needed)
4. **Similarity Search**: Computes cosine similarity between text and all frames
5. **Results**: Returns top-K most similar frames

## ğŸ”§ Technical Details

### Models Used

- **CLIP**: `openai/clip-vit-large-patch14` for image-text matching
- **Translation**: `Helsinki-NLP/opus-mt-ar-en` for Arabicâ†’English translation

### Key Improvements

âœ… **YouTube Download Support**: Added `yt-dlp` integration for direct YouTube video processing  
âœ… **Arabic Language Support**: Automatic translation from Arabic to English  
âœ… **Better Error Handling**: Comprehensive error messages and status updates  
âœ… **Larger CLIP Model**: Upgraded from base to large model for better accuracy  
âœ… **Frame Validation**: Checks for successful video opening and frame extraction  

## ğŸ¯ Use Cases

- ğŸ“º Content creators finding specific scenes in long videos
- ğŸ¬ Video editors searching for footage
- ğŸ“š Researchers analyzing video content
- ğŸ“ Students extracting key moments from lectures
- ğŸ” Anyone needing quick video content search

